import os
import itertools

if 'TEA_PATH' not in os.environ:
    exit("please defined TEA_PATH, the path of the direct path of the code folder")

import sys
import re

sys.path.insert(0, os.path.join(os.environ['TEA_PATH'], "code/features"))

from Note import Note

from utilities.timeml_utilities import get_text
from utilities.timeml_utilities import get_text_with_taggings
from utilities.timeml_utilities import get_tagged_entities
from utilities.timeml_utilities import get_text_element
from utilities.timeml_utilities import get_tlinks
from utilities.timeml_utilities import get_make_instances
from utilities.timeml_utilities import get_doctime_timex

from utilities.xml_utilities import get_raw_text
from utilities.pre_processing import pre_processing
from utilities.add_discourse import get_temporal_discourse_connectives

from Features import Features

import copy
from string import whitespace

import xml_utilities

class TimeNote(Note, Features):

    def __init__(self, timeml_note_path, annotated_timeml_path=None, verbose=False):

        if verbose: print "called TimeNote constructor"

        _Note = Note.__init__(self, timeml_note_path, annotated_timeml_path)
        _Features = Features.__init__(self)

        self._pre_process(timeml_note_path)

        self.iob_labels = None

        if self.annotated_note_path is not None:

            self.get_tlinked_entities()

            # will store labels in self.iob_labels
            self.get_iob_labels()

        else:
            self.tlinks = None

    def _pre_process(self, timeml_note_path):

        # callls newsreader pipeline, tokenizes, get_features
        data = get_text(timeml_note_path)

        # original text body of timeml doc
        self.original_text = data

        # sent text to newsreader for processing
        tokenized_text, token_to_offset, sentence_features = pre_processing.pre_process(data)

        # contains a lot of extra information generated by newsreader, not plain tokens
        self.pre_processed_text = tokenized_text

        # contains the char based offsets generated by tokenizer, used for asserting char offsets are correct
        self.token_to_offset = token_to_offset

        # contains sentence level information extracted by newsreader
        self.sentence_features = sentence_features

        # remove all the junk and just get list of list of tokens
        #self.extract_tokenized_text


    def get_timex_iob_labels(self):
        return self.filter_iob_by_type('TIMEX3')

    def get_event_iob_labels(self):
        return self.filter_iob_by_type('EVENT')

    def filter_iob_by_type(self, entity_type):
        assert entity_type in ['EVENT', 'TIMEX3']

        iob_labels = copy.deepcopy(self.get_iob_labels())

        for line in iob_labels:
            for iob_tag in line:

                if iob_tag["entity_type"] != entity_type:
                    iob_tag['entity_label'] = 'O'

        return iob_labels

    def set_tlinks(self, timexEventFeats, timexEventLabels, timexEventOffsets):

        # there should be no tlinks if this method is called.
        assert self.tlinks is None

        id_chunk_map = {}

        event_ids = set()
        timex_ids = set()

        chunks = []
        chunk = []

        id_chunks = []
        id_chunk = []

        start_entity_id = None

        B_seen = False

        for token, label in zip(timexEventFeats, timexEventLabels):

            # start of entity
            if re.search('^B_', label["entity_label"]):

                B_seen = True

                if label["entity_type"] == "EVENT":
                    event_ids.add(label["entity_id"])
                elif label["entity_type"] == "TIMEX3":
                    timex_ids.add(label["entity_id"])
                else:
                    exit("invalid type")

                if len(chunk) != 0:
                    chunks.append(chunk)
                    id_chunks.append(id_chunk)

                    assert start_entity_id not in id_chunk_map

                    id_chunk_map[start_entity_id] = chunk

                    chunk = [token]
                    id_chunk = [label["entity_id"]]

                else:
                    chunk.append(token)
                    id_chunk.append(label["entity_id"])

                start_entity_id = label["entity_id"]

            elif re.search('^I_', label["entity_label"]):

                if B_seen is False:
                    # TODO: put logic in to handle this.
                    exit("I label occured before B label")
        #            continue

                #token["entity_id"] = start_entity_id

                chunk.append(token)
                id_chunk.append(label["entity_id"])

            else:
                continue

        if len(chunk) != 0:
            chunks.append(chunk)
            assert len(id_chunk) == len(chunk)
            id_chunks.append(id_chunk)

            assert start_entity_id not in id_chunk_map
            id_chunk_map[start_entity_id] = chunk

        # add doc time. this is a timex.
        # TODO: need to add features for doctime. there aren't any.
        doctime = get_doctime_timex(self.note_path)
        doctime_id = doctime.attrib["tid"]
        doctime_dict = {}

        # create dict representation of doctime timex
        for attrib in doctime.attrib:

            doctime_dict[attrib] = doctime.attrib[attrib]

        id_chunk_map[doctime_id] = [doctime_dict]

        timex_ids.add(doctime_id)

        # cartesian product of entity pairs
        entity_pairs = filter(lambda t: t[0] != t[1], list(itertools.product(event_ids, timex_ids)) +\
                                                      list(itertools.product(timex_ids, event_ids)) +\
                                                      list(itertools.product(event_ids, event_ids)) +\
                                                      list(itertools.product(timex_ids, timex_ids)))

        entity_pairs = set(entity_pairs)

        relation_count = 0

        pairs_to_link = []

        for pair in entity_pairs:

            src_id = pair[0]
            target_id = pair[1]

            if "sentence_num" in id_chunk_map[src_id][0] and "sentence_num" in id_chunk_map[target_id][0]:

                src_sentence_num = id_chunk_map[src_id][0]["sentence_num"]
                target_sentence_num = id_chunk_map[target_id][0]["sentence_num"]

                if src_sentence_num != target_sentence_num and\
                   src_sentence_num + 1 != target_sentence_num:
                    continue

            # no link at all
            pairs_to_link.append({"src_entity":id_chunk_map[src_id], "src_id":src_id, "target_id":target_id, "target_entity":id_chunk_map[target_id], "rel_type":'None', "tlink_id":None})

#        assert len(pairs_to_link) == len(entity_pairs)

        self.tlinks = pairs_to_link



    def get_tlinked_entities(self):

        t_links = None

        if self.annotated_note_path is not None:
            t_links = get_tlinks(self.annotated_note_path)
            make_instances = get_make_instances(self.annotated_note_path)
        else:
            print "no annotated timeml note to get tlinks from returning empty list..."
            self.tlinks = []

        temporal_relations = {}

        eiid_to_eid = {}

        for instance in make_instances:
            eiid_to_eid[instance.attrib["eiid"]] = instance.attrib["eventID"]

        gold_tlink_pairs = set()

        for t in t_links:

            link = {}

            # source
            if "eventInstanceID" in t.attrib:
                src_id = eiid_to_eid[t.attrib["eventInstanceID"]]
            else:
                src_id = t.attrib["timeID"]

            # target
            if "relatedToEventInstance" in t.attrib:
                target_id = eiid_to_eid[t.attrib["relatedToEventInstance"]]
            else:
                target_id = t.attrib["relatedToTime"]

            tmp_dict = {"target_id":target_id, "rel_type":t.attrib["relType"], "lid":t.attrib["lid"]}

            gold_tlink_pairs.add((src_id, target_id))

            if src_id in temporal_relations:


                # this would mean the same src id will map to same target with different relation type.
                # not possible.
                assert tmp_dict not in temporal_relations[src_id]

                temporal_relations[src_id].append(tmp_dict)

            else:
                temporal_relations[src_id] = [tmp_dict]

        assert( len(gold_tlink_pairs) == len(t_links) )

        event_ids = set()
        timex_ids = set()

        chunks = []
        chunk = []

        id_chunk = []
        id_chunks = []

        start_entity_id = None

        id_chunk_map = {}

        B_seen = False

        # get tagged entities and group into a list
        for sentence_num, labels in zip(self.pre_processed_text, self.get_iob_labels()):

            for token, label in zip(self.pre_processed_text[sentence_num], labels):

                # start of entity
                if re.search('^B_', label["entity_label"]):

                    if label["entity_type"] == "EVENT":
                        event_ids.add(label["entity_id"])
                    else:
                        timex_ids.add(label["entity_id"])

                    if len(chunk) != 0:
                        chunks.append(chunk)
                        id_chunks.append(id_chunk)

                        assert start_entity_id not in id_chunk_map

                        id_chunk_map[start_entity_id] = chunk

                        chunk = [token]
                        id_chunk = [label["entity_id"]]

                    else:
                        chunk.append(token)
                        id_chunk.append(label["entity_id"])

                    start_entity_id = label["entity_id"]

                    B_seen = True

                elif re.search('^I_', label["entity_label"]):

                    assert label["entity_id"] == start_entity_id, "{} != {}, B_seen is {}".format(label["entity_id"], start_entity_id, B_seen)

                    chunk.append(token)
                    id_chunk.append(label["entity_id"])

                else:
                    pass

        if len(chunk) != 0:
            chunks.append(chunk)
            assert len(id_chunk) == len(chunk)
            id_chunks.append(id_chunk)

            assert start_entity_id not in id_chunk_map
            id_chunk_map[start_entity_id] = chunk

        chunk = []
        id_chunk = []

        assert len(event_ids.union(timex_ids)) == len(id_chunks)
        assert len(id_chunk_map.keys()) == len(event_ids.union(timex_ids))

        # TODO: need to add features for doctime. there aren't any.
        # add doc time. this is a timex.
        doctime = get_doctime_timex(self.note_path)
        doctime_id = doctime.attrib["tid"]
        doctime_dict = {}

        # create dict representation of doctime timex
        for attrib in doctime.attrib:

            doctime_dict[attrib] = doctime.attrib[attrib]

        id_chunk_map[doctime_id] = [doctime_dict]

        timex_ids.add(doctime_id)

        # cartesian product of entity pairs
        entity_pairs = list(itertools.product(event_ids, timex_ids)) +\
                      list(itertools.product(timex_ids, event_ids)) +\
                      list(itertools.product(event_ids, event_ids)) +\
                      list(itertools.product(timex_ids, timex_ids))

        entity_pairs = set(entity_pairs)

        relation_count = 0

        pairs_to_link = []

        tlink_ids = []

        for pair in entity_pairs:

            src_id = pair[0]
            target_id = pair[1]

            if "sentence_num" in id_chunk_map[src_id][0] and "sentence_num" in id_chunk_map[target_id][0]:

                src_sentence_num = id_chunk_map[src_id][0]["sentence_num"]
                target_sentence_num = id_chunk_map[target_id][0]["sentence_num"]

                if src_sentence_num != target_sentence_num and\
                   src_sentence_num + 1 != target_sentence_num:
                    continue

            pair = {"src_entity":id_chunk_map[src_id],
                    "src_id":src_id,
                    "target_id":target_id,
                    "target_entity":id_chunk_map[target_id],
                    "rel_type":'None',
                    "tlink_id":None}

            if src_id in temporal_relations:

                relation_found = False

                for target_entity in temporal_relations[src_id]:

                    if target_id == target_entity["target_id"]:

                        relation_count += 1

                        # need to assign relation to each pairing if there exists one otherwise set 'none'
                        pair["rel_type"] = target_entity["rel_type"]

                        # need to simplify tlinks

                        if pair["rel_type"] in ["IDENTITY", "DURING"]:
                            pair["rel_type"] = "SIMULTANEOUS"
                        elif pair["rel_type"] == "IBEFORE":
                            pair["rel_type"] = "BEFORE"
                        elif pair["rel_type"] == "IAFTER":
                            pair["rel_type"] = "AFTER"
                        elif pair["rel_type"] == "INCLUDES":
                            pair["rel_type"] = "IS_INCLUDED"
                        elif pair["rel_type"] == "BEGINS":
                            pair["rel_type"] = "BEGUN_BY"
                        elif pair["rel_type"] == "ENDS":
                            pair["rel_type"] = "ENDED_BY"
                        elif pair["rel_type"] not in [
                                                    "BEGUN_BY",
                                                    "IS_INCLUDED",
                                                    "AFTER",
                                                    "ENDED_BY",
                                                    "SIMULTANEOUS",
                                                    "DURING",
                                                    "IDENTITY",
                                                    "BEFORE"
                                                 ]:
                            print "rel_type: ", pair["rel_type"]
                            exit("unknown rel_type")

                        pair["tlink_id"] = target_entity["lid"]

                        tlink_ids.append(target_entity["lid"])

                        # done
                        break

            # no link at all
            pairs_to_link.append(pair)

        # TODO: if this fails just remove the assertion...
        # make sure we don't miss any tlinks
        assert relation_count == len(t_links), "{} != {}".format(relation_count, len(t_links))

        self.tlinks = pairs_to_link

    def get_iob_labels(self):


        if self.annotated_note_path is not None and self.iob_labels is None:

            # don't want to modify original
            pre_processed_text = copy.deepcopy(self.pre_processed_text)

            # need to create a list of tokens
            iob_labels = []

            tagged_entities = get_tagged_entities(self.annotated_note_path)
            _tagged_entities = copy.deepcopy(tagged_entities)

            raw_text = get_text(self.note_path)
            labeled_text = get_text_with_taggings(self.annotated_note_path)

            # lots of checks!
            for char in ['\n'] + list(whitespace):
                raw_text     = raw_text.strip(char)
                labeled_text = labeled_text.strip(char)

            raw_text     = xml_utilities.strip_quotes(raw_text)
            labeled_text = xml_utilities.strip_quotes(labeled_text)

            raw_text = re.sub("<TEXT>\n+", "", raw_text)
            raw_text = re.sub("\n+</TEXT>", "", raw_text)

            labeled_text = re.sub("<TEXT>\n+", "", labeled_text)
            labeled_text = re.sub("\n+</TEXT>", "", labeled_text)

            raw_index = 0
            labeled_index = 0

            raw_char_offset = 0
            labeled_char_offset = 0

            # should we count?
            count_raw = True
            count_labeled = True

            text1 = ""
            text2 = ""

            start_count = 0
            end_count = 0

            offsets = {}

            tagged_element = None

            # need to get char based offset for each tagging within annotated timeml doc.
            while raw_index < len(raw_text) or labeled_index < len(labeled_text):

                if raw_index < len(raw_text):
                    if count_raw is True:
                        raw_char_offset += 1
                        text1 += raw_text[raw_index]
                    raw_index += 1

                if labeled_index < len(labeled_text):

                    if labeled_text[labeled_index:labeled_index+1] == '<' and labeled_text[labeled_index:labeled_index+2] != '</':

                        tagged_element = tagged_entities.pop(0)

                        count_labeled = False
                        start_count += 1

                    elif labeled_text[labeled_index:labeled_index+2] == '</':
                        count_labeled = False
                        start_count += 1

                    if labeled_text[labeled_index:labeled_index+1] == ">":

                        if tagged_element != None:

                            start = labeled_char_offset
                            end   = labeled_char_offset+len(tagged_element.text) - 1

                            # spans should be unique?
                            offsets[(start, end)] = {"tagged_xml_element":tagged_element, "text":tagged_element.text}

                            # ensure the text at the offset is correct
                            assert raw_text[start:end + 1] == tagged_element.text, "\'{}\' != \'{}\'".format( raw_text[start:end + 1], tagged_element.text)
                            tagged_element = None

                        end_count += 1
                        count_labeled = True

                        labeled_index += 1
                        continue

                    if count_labeled is True:
                        labeled_char_offset += 1
                        text2 += labeled_text[labeled_index]

                    labeled_index += 1

            assert text1 == text2, "{} != {}".format(text1, text2)
            assert start_count == end_count, "{} != {}".format(start_count, end_count)
            assert raw_index == len(raw_text) and labeled_index == len(labeled_text)
            assert raw_char_offset == labeled_char_offset
            assert len(tagged_entities) == 0
            assert tagged_element is None
            assert len(offsets) == len(_tagged_entities)

            for sentence_num in sorted(pre_processed_text.keys()):

                # list of dicts
                sentence = pre_processed_text[sentence_num]

                # iobs in a sentence
                iobs_sentence = []

                # need to assign the iob labels by token index
                for token in sentence:

                    # set proper iob label to token
                    iob_label, entity_type, entity_id = TimeNote.get_iob_label(token, offsets)

                    if iob_label is not 'O':
                        assert entity_id is not None
                        assert entity_type in ['EVENT', 'TIMEX3']
                    else:
                        assert entity_id is None
                        assert entity_type is None

                    iobs_sentence.append({'entity_label':iob_label,
                                          'entity_type':entity_type,
                                          'entity_id':entity_id})

                iob_labels.append(iobs_sentence)

            self.iob_labels = iob_labels

        return self.iob_labels

    def get_iob_features(self):

        """ returns featurized representation of events and timexes """

        vectors = []

        for line in self.pre_processed_text:

            for token in self.pre_processed_text[line]:

                token_features = self.get_features_for_token(token)
                vectors.append(token_features)

        return vectors

    def get_tokens(self):

        tokens = []

        for line in self.pre_processed_text:

            for token in self.pre_processed_text[line]:

                tokens.append(token)

        return tokens

    def set_iob_labels(self, iob_labels):

        self.iob_labels = iob_labels

    def get_tokenized_text(self):

        return self.pre_processed_text

    def get_features_for_token(self, token):
        """ get the features for given token

            token: a dictionary with various information
        """

        """
        TODO: add substantial features
        """

        features = {}

        features.update(self.get_text(token))
        features.update(self.get_ngram_features(token))
        features.update(self.get_pos_tag(token))
        features.update(self.get_lemma(token))
        features.update(self.get_ner_features(token))

        return features

    def get_ner_features(self, token):

        if "ner_tag" in token:

            return {"ner_tag":token["ner_tag"],
                    "ne_id":token["ne_id"]}

        # TODO: what problems might arise from labeling tokens as none if no tagging?, we'll find out!
        else:

            return {"ner_tag":'None',
                    "ne_id":'None'}



    def get_text(self, token):

        if "text" in token:

            return {"text":token["token"]}

        else:

            return {"text":"DATE"}

    def get_pos_tag(self, token):

        if "pos_tag" in token:

            return {"pos_tag":token["pos_tag"]}

        else:

            # creation time.
            return {"pos_tag":"DATE"}

    def get_lemma(self, token):

        if "pos_tag" in token:

            return {"lemma":token["lemma"]}

        else:

            # creation time
            # TODO: make better?
            return {"lemma":"DATE"}

    def get_ngram_features(self, token):

        features = {}

        features.update(self.get_tokens_to_right(token, span=3))
        features.update(self.get_tokens_to_left(token, span=3))

        return features

    def get_tokens_to_right(self, token, span):
        """ get the tokens to the right of token

            obtains tokens relative to the right of the current position of token
        """

        # TODO: set none if there are no tokens?

        token_offset = token["token_offset"]
        line = self.pre_processed_text[token["sentence_num"]]

        # make sure we got the right token
        assert  line[token_offset] == token
        assert span > 0, "set to one or more, otherwise it will just get the token itself"

        start = token_offset
        end   = start + 1 + span

        right_tokens = line[start:end][1:]

        tokens = dict([("right_token_{}".format(i+1), token["token"]) for i, token in enumerate(right_tokens)])

        return tokens

    def get_tokens_to_left(self, token, span):
        """ get the tokens to the left of token

            obtains tokens relative to the left of the current position of token
        """

        # TODO: set none if there are no tokens?
        token_offset = token["token_offset"]
        line = self.pre_processed_text[token["sentence_num"]]

        # make sure we got the right token
        assert  line[token_offset] == token
        assert span > 0, "set to one or more, otherwise it will just get the token itself"

        start   = token_offset - span
        end     = token_offset

        left_tokens = line[start:end]
        tokens = dict([("left_token_{}".format(i+1), token["token"]) for i, token in enumerate(left_tokens)])

        return tokens


    def get_tlink_ids(self):

        tlink_ids = []

        for tlink in self.tlinks:

            tlink_ids.append(tlink["tlink_id"])

        return tlink_ids

    def get_tlink_labels(self):
        """ return the labels of each tlink from annotated doc """

        tlink_labels = []

        for tlink in self.tlinks:

            tlink_labels.append(tlink["rel_type"])

        return tlink_labels

    def get_tlink_id_pairs(self):

        """ returns the id pairs of two entities joined together """

        tlink_id_pairs = []

        for tlink in self.tlinks:

            tlink_id_pairs.append((tlink["src_id"], tlink["target_id"]))

        return tlink_id_pairs

    def get_tlink_features(self):

        """
        TODO: add more substantial features
            -read paper
        """

        """ returns featurized representation of tlinks """

        print "called get_tlink_features"

        vectors = []

        for relation in self.tlinks:

            vector = {}

            target_entity = relation["target_entity"]
            src_entity = relation["src_entity"]

            vector = self.get_features_for_entity_pair(src_entity, target_entity)

            vectors.append(vector)

        return vectors


    def get_features_for_entity_pair(self, src_entity, target_entity):

        """ get the features for an entity pair
        """

        pair_features = {}

        src_features = {}
        target_features = {}

        # features for each entity
        src_features.update(self.get_entity_type_features(src_entity))
        src_features.update(self.get_label_features(src_entity))
        src_features.update(self.get_text_features(src_entity))

        target_features.update(self.get_entity_type_features(target_entity))
        target_features.update(self.get_label_features(target_entity))
        target_features.update(self.get_text_features(target_entity))

        # features concerning each entity
        pair_features.update(self.get_same_pos_tag_feature(src_entity, target_entity))
        pair_features.update(self.get_sentence_distance_feature(src_entity, target_entity))
        #pair_features.update(self.get_discourse_connectives_features(src_entity, target_entity))

        for key in src_features:

            pair_features[key + "_src"] = src_features[key]

        for key in target_features:

            pair_features[key + "_target"] = target_features[key]

        return pair_features

    def get_sentence_distance_feature(self, src_entity, target_entity):

        src_line_no = None
        target_line_no = None

        for token in src_entity:

            if src_line_no is None:

                if "sentence_num" in token:
                    src_line_no = token["sentence_num"]
                else:
                    # creation time is not in a sentence.
                    return {"sent_distance":'None'}

            else:

                assert token["sentence_num"] == src_line_no

        for token in target_entity:

            if target_line_no is None:

                if "sentence_num" in token:
                    target_line_no = token["sentence_num"]
                else:
                    # creation time is not in a sentence.
                    return {"sent_distance":'None'}

            else:

                assert token["sentence_num"] == target_line_no

        return {"sent_distance":src_line_no - target_line_no}


    def get_discourse_connectives_features(self, src_entity, target_entity):
        ''' return tokens of temporal discourse connectives and their distance from each entity, if connective exist and entities are on the same line.'''


        src_line_no = None
        target_line_no = None

        for token in src_entity:

            if src_line_no is None:

                if "sentence_num" in token:
                    src_line_no = token["sentence_num"]
                else:
                    # creation time is not in a sentence.
                    return {"sent_distance":'None'}

            else:

                assert token["sentence_num"] == src_line_no

        for token in target_entity:

            if target_line_no is None:

                if "sentence_num" in token:
                    target_line_no = token["sentence_num"]
                else:
                    # creation time is not in a sentence.
                    return {"sent_distance":'None'}

            else:

                assert token["sentence_num"] == target_line_no

        if src_line_no is target_line_no:

            connectives = self.get_discourse_connectives(src_line_no)



        return{"connective_tokens": 'None', "connective_distance_from_src":'None', "connective_distance_from_target": 'None'}



    def get_discourse_connectives(self, line_no):

        constituency_tree = self.sentence_features[line_no]['constituency_tree']

        connectives = get_temporal_discourse_connectives(constituency_tree)

        return connectives


    def get_text_features(self, entity):

        """
            gets the features for entity:

                pos
                text of tokens
                lemma of tokens
                text chunk of entity

        """

        features = {}

        tokens = []

        for i, token in enumerate(entity):

            features.update({"lemma_{}".format(i):self.get_lemma(token)["lemma"]})

            tokens.append(self.get_text(token)["text"])

            features.update({"text_{}".format(i):self.get_text(token)["text"]})
            features.update({"pos_{}".format(i):self.get_pos_tag(token)["pos_tag"]})

        features.update({"chunk":" ".join(tokens)})

        return features

    def get_same_pos_tag_feature(self, src_entity, target_entity):

        src_pos_tags = []
        target_pos_tags = []

        for token in src_entity:

            src_pos_tags.append(self.get_pos_tag(token)["pos_tag"])

        for token in target_entity:

            target_pos_tags.append(self.get_pos_tag(token)["pos_tag"])

        return {"same_pos_tags":src_pos_tags == target_pos_tags}


    def get_label_features(self, entity):

        features = {}

        for i, token in enumerate(entity):

            features.update({"label_type{}".format(i):self.token_label_feature(token)["entity_label"]})

        return features

    def get_entity_type_features(self, entity):

        """ for some entity, get the entity type labeling for tokens in that entity
        """

        features = {}

        for i, token in enumerate(entity):

            features.update({"entity_type{}".format(i):self.token_entity_type_feature(token)["entity_type"]})

        return features

    def token_label_feature(self, token):

        feature = {}
        feature = {}

        line_num = None
        token_offset = None

        label = None

        # TODO: correct this, hacky
        if "functionInDocument" in token:
            # this is the creation time...
            label = "B_DATE"

        else:

            line_num     = token["sentence_num"] - 1
            token_offset = token["token_offset"]

            iob_labels  =  self.get_iob_labels()
            label = iob_labels[line_num][token_offset]["entity_label"]

        assert label not in ['O', None]

        return {"entity_label":label}


    def token_entity_type_feature(self, token):

        """ for some token, get the entity type (EVENT or TIMEX3) for it
        """

        feature = {}

        line_num = None
        token_offset = None

        entity_type = None

        # TODO: correct this, hacky
        if "functionInDocument" in token:
            # this is the creation time...
            entity_type = "TIMEX3"

        else:

            line_num     = token["sentence_num"] - 1
            token_offset = token["token_offset"]

            iob_labels  =  self.get_iob_labels()
            entity_type = iob_labels[line_num][token_offset]["entity_type"]

        assert entity_type in ["EVENT", "TIMEX3"]

        return {"entity_type":entity_type}


    def get_token_char_offsets(self):

        """ returns the char based offsets of token.

        for each token within self.pre_processed_text iterate through list of dicts
        and for each value mapped to the key 'start_offset' and 'end_offset' create a
        list of 1-1 mappings

        Returns:
            A flat list of offsets of the token within self.pre_processed_text:

                [(0,43),...]
        """

        offsets = []

        for line_num in self.pre_processed_text:

            for token in self.pre_processed_text[line_num]:

                offsets.append((token["char_start_offset"], token["char_end_offset"]))

        return offsets

    @staticmethod
    def get_iob_label(token, offsets):

        # NOTE: never call this directly. input is tested within _read
        tok_span = (token["char_start_offset"], token["char_end_offset"])

        label = 'O'
        entity_id = None
        entity_type = None

        for span in offsets:

            if offsets[span]["tagged_xml_element"].tag not in ["EVENT", "TIMEX3"]:
                continue

            if TimeNote.same_start_offset(span, tok_span):

                labeled_entity = offsets[span]["tagged_xml_element"]

                if 'class' in labeled_entity.attrib:
                    label = 'B_' + labeled_entity.attrib["class"]
                elif 'type' in labeled_entity.attrib:
                    label = 'B_' + labeled_entity.attrib["type"]

                if 'eid' in labeled_entity.attrib:
                    entity_id = labeled_entity.attrib["eid"]
                else:
                    entity_id = labeled_entity.attrib["tid"]

                entity_type = labeled_entity.tag

                break

            elif TimeNote.subsumes(span, tok_span):

                labeled_entity = offsets[span]["tagged_xml_element"]

                if 'class' in labeled_entity.attrib:
                    label = 'I_' + labeled_entity.attrib["class"]
                else:
                    label = 'I_' + labeled_entity.attrib["type"]

                if 'eid' in labeled_entity.attrib:
                    entity_id = labeled_entity.attrib["eid"]
                else:
                    entity_id = labeled_entity.attrib["tid"]

                entity_type = labeled_entity.tag

                break

        return label, entity_type, entity_id

    @staticmethod
    def same_start_offset(span1, span2):
        """
        doees span1 share the same start offset?
        """
        return span1[0] == span2[0]

    @staticmethod
    def subsumes(span1, span2):
        """
        does span1 subsume span2?
        """
        return span1[0] < span2[0] and span2[1] <= span1[1]


def __unit_tests():

    """ basic assertions to ensure output correctness """

    t =  TimeNote("APW19980219.0476.tml.TE3input", "APW19980219.0476.tml")

    for label in t.get_timex_iob_labels():
        for token in label:

            if token['entity_type'] == 'TIMEX3':
                assert token['entity_label'] != 'O'
            else:
                assert token['entity_label'] == 'O'

    for label in t.get_event_iob_labels():
        for token in label:

            if token['entity_type'] == 'EVENT':
                assert token['entity_label'] != 'O'
            else:
                assert token['entity_label'] == 'O'

    """
    number_of_tlinks = len(t.get_tlink_features())
    assert number_of_tlinks != 0
    assert len(t.get_tlink_id_pairs()) == number_of_tlinks, "{} != {}".format(len(t.get_tlink_id_pairs()), number_of_tlinks)
    assert len(t.get_tlink_labels()) == number_of_tlinks
    assert len(t.get_tlink_ids()) == number_of_tlinks
    #prin t.get_token_char_offsets()
    """

    t.get_tlink_features()

#    print t.get_iob_features()

#    print t.get_tlinked_entities()

#    print t.get_tlink_labels()

if __name__ == "__main__":

    __unit_tests()

    print "nothing to do"




