import os
import itertools
import sys
import re
import copy
import numpy as np

from string import whitespace
from Note import Note

from utilities.timeml_utilities import annotate_root
from utilities.timeml_utilities import annotate_text_element
from utilities.timeml_utilities import get_doctime_timex
from utilities.timeml_utilities import get_make_instances
from utilities.timeml_utilities import get_stripped_root
from utilities.timeml_utilities import get_tagged_entities
from utilities.timeml_utilities import get_text
from utilities.timeml_utilities import get_text_element
from utilities.timeml_utilities import get_text_element_from_root
from utilities.timeml_utilities import get_text_with_taggings
from utilities.timeml_utilities import get_tlinks
from utilities.timeml_utilities import set_text_element

from utilities.xml_utilities import get_raw_text
from utilities.xml_utilities import get_root
from utilities.xml_utilities import write_root_to_file

# from utilities.time_norm import get_normalized_time_expressions
from utilities.pre_processing import pre_processing

class TimeNote(Note):

    def __init__(self, timeml_note_path, annotated_timeml_path=None, verbose=False):

        if verbose: print "called TimeNote constructor"

        _Note = Note.__init__(self, timeml_note_path, annotated_timeml_path)

        # # get body of document
        # data = get_text(timeml_note_path)

        # original text body of timeml doc
        self.original_text = get_text(timeml_note_path)

        # send body of document to NewsReader pipeline.
        tokenized_text, token_to_offset, sentence_features, dependency_paths, id_to_tok = pre_processing.pre_process(self.original_text)

        # {sentence_num: [{token},...], ...}
        self.pre_processed_text = tokenized_text

        # contains the char based offsets generated by tokenizer, used for asserting char offsets are correct
        # {'token':[(start, end),...],...}
        self.token_to_offset = token_to_offset

        # contains sentence level information extracted by newsreader
        self.sentence_features = sentence_features

        # dependency paths for sentences in the document
        self.dependency_paths = dependency_paths

        # map token ids to tokens within self.tokenized_text
        # {'wid':'token'}
        self.id_to_tok = id_to_tok

        self.discourse_connectives = {}

        self.iob_labels = []
        self.event_ids = set([])
        self.timex_ids = set([])
        self.id_to_labels = {} # map from id pairs to tlink labels. Negative not included.

        """
        print "\n\nself.original_text:\n\n"
        print self.original_text
        print "\n\n"

        print "self.pre_processed_text:\n\n"
        print tokenized_text
        print "\n\n"

        print "self.token_to_offset:\n\n"
        print self.token_to_offset
        print "\n\n"

        print "self.sentence_features:\n\n"
        print self.sentence_features
        print "\n\n"
        """

        self.tlinks = []
        self.get_id_word_map()
        self.get_valid_pairs()

        if self.annotated_note_path is not None:

            self.get_tlinked_entities()

            # will store labels in self.iob_labels
            self.get_labels()
            self.get_id_to_labels()
            #self.get_neg_data_indexes()
            self.get_neg_pairs()

    def get_tlinks(self):
        print "get tlinks from:", self.annotated_note_path
        return get_tlinks(self.annotated_note_path)

    def get_sentence_features(self):
        return self.sentence_features

    def get_tokenized_text(self):
        return self.pre_processed_text

    def get_discourse_connectives(self):
        return self.discourse_connectives

    def add_discourse_connectives(self, connectives):
        self.discourse_connectives.update(connectives)

    def get_timex_labels(self):
        return self.filter_label_by_type('TIMEX3')

    def get_event_labels(self):

        labels = self.filter_label_by_type("EVENT")

        for line in labels:
            for label in line:
                if label["entity_type"] != "EVENT":
                    label['entity_label'] = 'O'
                else:
                    label['entity_label'] = "EVENT"

        return labels

    def get_event_class_labels(self):
         return self.filter_label_by_type('EVENT')

    def filter_label_by_type(self, entity_type):
        assert entity_type in ['EVENT', 'TIMEX3']

        labels = copy.deepcopy(self.get_labels())

        for line in labels:
            for label in line:

                if label["entity_type"] != entity_type:
                    label['entity_label'] = 'O'

        return labels

    def set_tlinked_entities(self, timexLabels, eventClassLabels):
        """
            Set the tlink entities given the taggings from classifiers.
            This is used during prediction prior to feature extraction of tlink pairings.
            NOTE: this function will modify the dictionaries within timexLabels to correct I_ before B_ taggings.
        """

        # there should be no tlinks if this method is called.
        assert len(self.tlinks) == 0

        entity_pairs = self.get_entity_pairs()

        relation_count = 0

        pairs_to_link = []
        tlink_ids = []

        id_chunk_map, event_ids, timex_ids, sentence_chunks = self.get_id_chunk_map()

        # create the pair representation.
        for pair in entity_pairs:

            src_id = pair[0][1]
            target_id = pair[1][1]

            pair = {"src_entity":id_chunk_map[src_id],
                    "src_id":src_id,
                    "target_id":target_id,
                    "target_entity":id_chunk_map[target_id],
                    "rel_type":'None',
                    # no links!
                    "tlink_id":None}

            pairs_to_link.append(pair)

        self.tlinks = pairs_to_link

        return

    @staticmethod
    def transitive_closure(a):
        closure = set(a)
        while True:
            new_relations = set((x, w) for x, y in closure for q, w in closure if q == y)

            closure_until_now = closure | new_relations

            if closure_until_now == closure:
                break

            closure = closure_until_now

        return closure

    def get_closures(self):
        """
        Get closure of tlinks.
        We assume the class labels are not combined/converted so it can be general.
        @return:
        """
        raw_t_links = self.tlinks

        # BEFORE CLOSURE
        # Assuming using the augmented training data
        # so all AFTER pairs are also converted to BEFORE pairs already

        BEFORE_t_links = [(x['src_id'], x['target_id']) for x in raw_t_links if x['rel_type'] in ('BEFORE', 'IBEFORE')]
        #AFTER_t_links = [x for x in raw_t_links if x['rel_type'] in ('AFTER', 'IAFTER')]
        #INCLUDES_t_links = [x for x in raw_t_links if x['rel_type'] == 'INCLUDES']
        IS_INCLUDED_t_links = [x for x in raw_t_links if x['rel_type'] == 'IS_INCLUDED']
        SIMULTANEOUS_t_links = [x for x in raw_t_links if x['rel_type'] == 'SIMULTANEOUS']

        # for item in AFTER_t_links:
        #     new_item = {}
        #     if item['rel_type'] == 'AFTER':
        #         new_item['rel_type'] = 'BEFORE'
        #     else:
        #         new_item['rel_type'] = 'IBEFORE'
        #     new_item['src_entity'] = item['target_entity']
        #     new_item['src_id'] = item['target_id']
        #     new_item['target_entity'] = item['src_entity']
        #     new_item['target_id'] = item['src_id']
        #     new_item['tlink_id'] = item['tlink_id'] + '0000'
        #
        #     BEFORE_t_links.append(new_item)
        #
        # for item in INCLUDES_t_links:
        #     new_item = {}
        #     new_item['rel_type'] = 'IS_INCLUDED'
        #     new_item['src_entity'] = item['target_entity']
        #     new_item['src_id'] = item['target_id']
        #     new_item['target_entity'] = item['src_entity']
        #     new_item['target_id'] = item['src_id']
        #     new_item['tlink_id'] = item['tlink_id'] + '0000'
        #
        #     IS_INCLUDED_t_links.append(new_item)

        # make tuples of src_id, target_id
        BEFORE_pairs = [(x['src_id'], x['target_id']) for x in BEFORE_t_links]
        BEFORE_closure = TimeNote.transitive_closure(BEFORE_pairs)
        AFTER_closure = set([(y,x) for (x, y) in BEFORE_closure])

        IS_INCLUDED_pairs = [(x['src_id'], x['target_id']) for x in IS_INCLUDED_t_links]
        IS_INCLUDED_closure = TimeNote.transitive_closure(IS_INCLUDED_pairs)


        # build tlinks from closure

        for i, item in enumerate(self.tlinks):
            if (item['src_id'], item['target_id']) in BEFORE_closure and item['rel_type'] == 'None': # do not change original label:
                self.tlinks[i]['rel_type'] = 'BEFORE'
            if (item['src_id'], item['target_id']) in AFTER_closure and item['rel_type'] == 'None':
                self.tlinks[i]['rel_type'] = 'AFTER'
            if (item['src_id'], item['target_id']) in IS_INCLUDED_closure and item['rel_type'] == 'None':
                self.tlinks[i]['rel_type'] = 'IS_INCLUDED'
        #self.tlinks += BEFORE_t_links
        #self.tlinks += IS_INCLUDED_t_links


    def get_tlinked_entities(self):

        #_LABELS_TO_FLIP = ["INCLUDES", "ENDS", "BEGINS"]
        _LABELS_TO_FLIP = []

        t_links = None

        if len(self.tlinks) > 0:
            return self.tlinks
        elif self.annotated_note_path is not None:
            t_links = get_tlinks(self.annotated_note_path)
            make_instances = get_make_instances(self.annotated_note_path)
        else:
            print "no annotated timeml note to get tlinks from returning empty list..."
            self.tlinks = []
            return self.tlinks

        temporal_relations = {}

        eiid_to_eid = {}

        for instance in make_instances:
            eiid_to_eid[instance.attrib["eiid"]] = instance.attrib["eventID"]

        gold_tlink_pairs = []

        # TODO: figure out how to handle the problem where a token occurs in two different make instances.
        count = 1
        for t in t_links:
            # print count, t.attrib
            link = {}

            # source
            if "eventInstanceID" in t.attrib:
                src_id = eiid_to_eid[t.attrib["eventInstanceID"]]
            else:
                src_id = t.attrib["timeID"]

            # target
            if "relatedToEventInstance" in t.attrib:
                target_id = eiid_to_eid[t.attrib["relatedToEventInstance"]]
            else:
                target_id = t.attrib["relatedToTime"]

            # We swap some labels for their directional inverse to help with sparsity of training data
            if t.attrib["relType"] in _LABELS_TO_FLIP:
                src_id, target_id = target_id, src_id

            tmp_dict = {"target_id":target_id, "rel_type":t.attrib["relType"], "lid":t.attrib["lid"]}

            gold_tlink_pairs.append((src_id, target_id))

            if src_id in temporal_relations:
                # this would mean the same src id will map to same target with different relation type.
                # not possible.
                assert tmp_dict not in temporal_relations[src_id]
                temporal_relations[src_id].append(tmp_dict)
            else:
                temporal_relations[src_id] = [tmp_dict]

            count += 1

        assert( len(gold_tlink_pairs) == len(t_links) ), "{} != {}".format(len(gold_tlink_pairs) , len(t_links))

        relation_count = 0

        pairs_to_link = []
        tlink_ids = []

        entity_pairs = self.get_entity_pairs()

        id_chunk_map, event_ids, timex_ids, sentence_chunks = self.get_id_chunk_map()

        for pair in entity_pairs:

            src_id = pair[0][1]
            target_id = pair[1][1]

            pair = {"src_entity":id_chunk_map[src_id],
                    "src_id":src_id,
                    "target_id":target_id,
                    "target_entity":id_chunk_map[target_id],
                    "rel_type":'None',
                    "tlink_id":None}


            if src_id in temporal_relations:

                # relation_found = False

                for temporal_relation in temporal_relations[src_id]:

                    if target_id == temporal_relation["target_id"]:

                        relation_count += 1

                        # need to assign relation to each pairing if there exists one otherwise set 'none'
                        pair["rel_type"] = temporal_relation["rel_type"]

                        # need to simplify tlinks

                        if pair["rel_type"]  in ("IDENTITY", "DURING"):
                            pair["rel_type"] = "SIMULTANEOUS"
                        elif pair["rel_type"] == "IBEFORE":
                            pair["rel_type"] = "BEFORE"
                        elif pair["rel_type"] == "IAFTER":
                            pair["rel_type"] = "AFTER"
                        # elif pair["rel_type"] == "INCLUDES": # already flipped
                        #     pair["rel_type"] = "IS_INCLUDED"
                        # elif pair["rel_type"] == "BEGINS": # already flipped
                        #     pair["rel_type"] = "BEGUN_BY"
                        # elif pair["rel_type"] == "ENDS": # already flipped
                        #     pair["rel_type"] = "ENDED_BY"
                        # elif pair["rel_type"] not in [
                        #                             "BEGUN_BY",
                        #                             "IS_INCLUDED",
                        #                             "AFTER",
                        #                             "ENDED_BY",
                        #                             "SIMULTANEOUS",
                        #                             "DURING",
                        #                             "IDENTITY",
                        #                             "BEFORE"
                        #                          ]:
                        #     print "rel_type: ", pair["rel_type"]
                        #     exit("unknown rel_type")

                        pair["tlink_id"] = temporal_relation["lid"]

                        tlink_ids.append(temporal_relation["lid"])

                        # done
                        break

            # no link at all
            pairs_to_link.append(pair)

        # for link in t_links:
        #     if link.attrib["lid"] not in tlink_ids:
        #         print "MISSING ", link.tag, ": ", link.attrib

        # TODO: if this fails just remove the assertion...
        # make sure we don't miss any tlinks (This fails due to cross document tlinks that fall outside the scope of our classifiers)
        # assert relation_count == len(t_links), "{} != {}".format(relation_count, len(t_links))

        self.tlinks = pairs_to_link
        #self.get_closures()

        self.eiid_to_eid = eiid_to_eid

        return self.tlinks

    def get_entity_pairs(self):

        id_chunk_map, event_ids, timex_ids, sentence_chunks = self.get_id_chunk_map()

        doctime = get_doctime_timex(self.note_path) # doc creation time
        doctime_id = doctime.attrib["tid"]
        self.doctime = doctime

        entity_pairs = []

        # TODO: make more efficient...
        for sentence_num in sentence_chunks:
            for i, entity in enumerate(sentence_chunks[sentence_num]):
                entity_id   = entity[1]
                entity_type = entity[0]

                if entity_type == "EVENT":
                    entity_pairs += list(itertools.product([entity], sentence_chunks[sentence_num][i+1:]))
                    entity_pairs.append((entity, ("TIMEX", doctime_id)))
                else:
                    events = map(lambda event: event, filter(lambda entity: entity[0] == "EVENT", sentence_chunks[sentence_num][i+1:]))
                    entity_pairs += list(itertools.product(events, [("TIMEX", entity_id)]))
                    # entity_pairs.append((entity, ("TIMEX", doctime_id)))

            if sentence_num + 1 in sentence_chunks:

                # get events of sentence
                event_ids = filter(lambda entity: entity[0] == "EVENT", sentence_chunks[sentence_num])
                main_events = filter(lambda event_id: True in [token.get("is_main_verb", False) for token in id_chunk_map[event_id[1]]], event_ids)
                main_events = map(lambda event: event, main_events)

                # get adjacent sentence events and filter the main events
                adj_event_ids = filter(lambda entity: entity == "EVENT", sentence_chunks[sentence_num+1])
                adj_main_events = filter(lambda event_id: True in [token.get("is_main_verb", False) for token in id_chunk_map[event_id[1]]], adj_event_ids)

                entity_pairs += list(itertools.product(main_events, adj_main_events))

        # add relations in the other direction (b -> a instead of a -> b)
        old_pairs = entity_pairs
        entity_pairs = []
        for pair in old_pairs:
            src = pair[0]
            target = pair[1]

            entity_pairs.append((src, target))
            entity_pairs.append((target, src))

        return entity_pairs

    def get_id_chunk_map(self):

        event_ids = set()
        timex_ids = set()

        chunks = []
        chunk = []

        id_chunk = []
        id_chunks = []

        start_entity_id = None

        id_chunk_map = {}

        B_seen = False

        sentence_chunks = {}
        previous_label = 'dummy_label'

        # get tagged entities and group into a list
        for sentence_num, labels in zip(self.pre_processed_text, self.get_labels()):

            sentence_chunks[sentence_num] = []

            for token, label in zip(self.pre_processed_text[sentence_num], labels):

                if label["entity_type"] == "EVENT":

                    if label["entity_id"] not in id_chunk_map:
                        _chunk = [token]
                        chunks.append(_chunk)

                        event_ids.add(label["entity_id"])

                        id_chunks.append([label["entity_id"]])
                        id_chunk_map[label["entity_id"]] = _chunk
                        sentence_chunks[sentence_num].append(("EVENT", label["entity_id"]))
                    else:
                        chunks[-1] = chunks[-1] + _chunk
                        id_chunk_map[label["entity_id"]] = chunks[-1]



                # in timex chunk
                if re.search('^I_', label["entity_label"]) and label["entity_type"] == 'TIMEX3':
                    # This assertion requires the I_DATE to be in the same entity started with previous B_DATE
                    # Ideally it should be true, but it often causes trouble
                    #assert label["entity_id"] == start_entity_id, "{} != {}, B_seen is {}".format(label["entity_id"], start_entity_id, B_seen)
                    if 'B_' in previous_label or 'I_' in previous_label:
                        if label["entity_id"] == start_entity_id:
                            chunk.append(token)
                            id_chunk.append(label["entity_id"])
                        else:
                            print "{} != {}, B_seen is {}".format(label["entity_id"], start_entity_id, B_seen)
                            label["entity_label"] = label["entity_label"].replace('I_', 'B_')
                    else:
                        label["entity_label"] = label["entity_label"].replace('I_', 'B_')

                # start of timex
                if re.search('^B_', label["entity_label"]) and label["entity_type"] == 'TIMEX3':

                    timex_ids.add(label["entity_id"])

                    if len(chunk) != 0:
                        chunks.append(chunk)
                        id_chunks.append(id_chunk)

                        assert start_entity_id not in id_chunk_map

                        #print "TIMEX: adding to id_chunk _map"
                        #print "\t", label

                        id_chunk_map[start_entity_id] = chunk

                        #if start_entity_id is None:
                        #    print "start_entity_id is NONE"
                        #    print label

                        sentence_chunks[sentence_num].append(("TIMEX", start_entity_id))

                        chunk = [token]
                        id_chunk = [label["entity_id"]]


                    else:
                        chunk.append(token)
                        id_chunk.append(label["entity_id"]) # id of the timex entity

                    start_entity_id = label["entity_id"] # set the start of timex entity

                    B_seen = True

                previous_label = label["entity_label"]

            if len(chunk) != 0:
                chunks.append(chunk)
                assert len(id_chunk) == len(chunk)
                id_chunks.append(id_chunk)

                try:
                    assert start_entity_id not in id_chunk_map
                except AssertionError:
                    print "found start_entity_id in id_chunk_map:", start_entity_id
                id_chunk_map[start_entity_id] = chunk

                sentence_chunks[sentence_num].append(("TIMEX", start_entity_id))

            chunk = []
            id_chunk = []

        # all number of entities should equal number of chunks
        assert len(event_ids.union(timex_ids)) == len(id_chunks), "{} != {}".format(len(event_ids.union(timex_ids)), len(id_chunks))
        assert len(id_chunk_map.keys()) == len(event_ids.union(timex_ids)), "{} != {}".format(len(id_chunk_map.keys()), len(event_ids.union(timex_ids)))

        # TODO: need to add features for doctime. there aren't any.
        # add doc time. this is a timex.
        doctime = get_doctime_timex(self.note_path)
        doctime_id = doctime.attrib["tid"]
        doctime_dict = {}

        # create dict representation of doctime timex
        for attrib in doctime.attrib:
            doctime_dict[attrib] = doctime.attrib[attrib]

        id_chunk_map[doctime_id] = [doctime_dict]
        timex_ids.add(doctime_id)


        return id_chunk_map, event_ids, timex_ids, sentence_chunks

    def get_id_word_map(self):
        # map eventID or timexID to wID
        self.id_to_wordIDs = {} # str -> list
        self.id_to_sent = {} # str -> int
        id_chunk_map, event_ids, timex_ids, sentence_chunks = self.get_id_chunk_map()
        self.event_ids = event_ids
        self.timex_ids = timex_ids
        for etid in id_chunk_map: # id to chunk of events
            self.id_to_wordIDs[etid] = [x.get('id', None) for x in id_chunk_map[etid]]
            self.id_to_sent[etid] = id_chunk_map[etid][0].get('sentence_num', None)

    def get_valid_pairs(self):
        self.intra_sentence_pairs = []
        self.cross_sentence_pairs = []
        self.dct_pairs = []
        self.timex_pairs = []
        for src_etid in self.id_to_wordIDs: # eventID/timexID -> words
            for target_etid in self.id_to_wordIDs: # we allow both (e1, e2) and (e2, e1)
                if src_etid == target_etid or src_etid == 't0':
                    continue
                if src_etid[0] == 't' and target_etid[0] == 't': # timex
                    self.timex_pairs.append((src_etid, target_etid))
                elif target_etid == 't0':    # (e, t0) pairs
                    self.dct_pairs.append((src_etid, 't0'))
                elif abs(self.id_to_sent[src_etid]-self.id_to_sent[target_etid]) == 1: # pairs of consec sentences
                    self.cross_sentence_pairs.append((src_etid, target_etid))
                elif self.id_to_sent[src_etid]-self.id_to_sent[target_etid] == 0:  # pairs in the same sentence
                    self.intra_sentence_pairs.append((src_etid, target_etid))

    def get_intra_sentence_subpaths(self):
        id_pair_to_path = {}
        for src_id, target_id in self.intra_sentence_pairs:
            src_wordID = self.id_to_wordIDs[src_id][0]
            target_wordID = self.id_to_wordIDs[target_id][0]
            # left path is always the target entity path
            left_path, right_path = self.dependency_paths.get_left_right_subpaths('t'+target_wordID[1:], 't'+src_wordID[1:])
            id_pair_to_path[(src_id, target_id)] = (left_path, right_path)

        return id_pair_to_path

    def get_intra_sentence_context(self):
        id_pair_to_context = {}
        for src_id, target_id in self.intra_sentence_pairs:
            src_wordID = self.id_to_wordIDs[src_id][0]
            target_wordID = self.id_to_wordIDs[target_id][0]
            source = self.id_to_tok[src_wordID]
            target = self.id_to_tok[target_wordID]

            sentence_num = source['sentence_num']

            if source['token_offset'] <= target['token_offset']:
                left = source
                right = target
            else:
                left = target
                right = source
            left_indexes, right_indexes = self._get_context(left['token_offset'], right['token_offset'],
                                                            len(self.pre_processed_text[sentence_num]))
            if source['token_offset'] <= target['token_offset']:
                # source vector, left means "source" here
                left_context = [token['token'] for token in
                                self.pre_processed_text[sentence_num][left_indexes[0]:left_indexes[1]]]
                # target vector, right means "target" here
                right_context = [token['token'] for token in
                                self.pre_processed_text[sentence_num][right_indexes[0]:right_indexes[1]]]
            else:
                right_context = [token['token'] for token in
                                     self.pre_processed_text[sentence_num][left_indexes[0]:left_indexes[1]]]
                left_context = [token['token'] for token in
                                  self.pre_processed_text[sentence_num][right_indexes[0]:right_indexes[1]]]

            id_pair_to_context[(src_id, target_id)] = (left_context, right_context)
        return id_pair_to_context

    def _get_context(self, left_offset, right_offset, max_len):
        l_context_l_edge = max(0, left_offset - 5)
        l_context_r_edge = min(right_offset+1, left_offset + 11) # cover the other entity too
        r_context_l_edge = max(left_offset, right_offset - 10) # cover the other entity too
        r_context_r_edge = min(right_offset + 6, max_len)
        return (l_context_l_edge, l_context_r_edge), (r_context_l_edge, r_context_r_edge)

    def find_root(self, word_id):
        if word_id[0] == 'w':
            word_id = 't' +word_id[1:]
        while True:
            try:
                parent = self.dependency_paths.tree[word_id].parent
            except KeyError:
                print self.annotated_note_path
                print "KeyError"
                print "word_id:", word_id
                return word_id
            if not parent:
                return word_id
            else:
                word_id = parent

    def get_cross_sentence_subpaths(self):
        id_pair_to_path = {}
        for src_id, target_id in self.cross_sentence_pairs:
            src_wordID = self.id_to_wordIDs[src_id][0]
            target_wordID = self.id_to_wordIDs[target_id][0]

            root_src = self.find_root(src_wordID)
            root_target = self.find_root(target_wordID)

            # get path for src
            left, right = self.dependency_paths.get_left_right_subpaths(root_src, 't'+src_wordID[1:])
            if len(left) > len(right):
                src_path = left
            else:
                src_path = right

            # get path for target
            left, right = self.dependency_paths.get_left_right_subpaths(root_target, 't'+target_wordID[1:])
            if len(left) > len(right):
                target_path = left
            else:
                target_path = right

            id_pair_to_path[(src_id, target_id)] = (target_path, src_path)

        return id_pair_to_path

    def get_cross_sentence_context(self):
        id_pair_to_context = {}
        left_context = []
        right_context = []
        for src_id, target_id in self.cross_sentence_pairs:
            src_wordID = self.id_to_wordIDs[src_id][0]
            target_wordID = self.id_to_wordIDs[target_id][0]
            source = self.id_to_tok[src_wordID]
            target = self.id_to_tok[target_wordID]

            src_sentence_num = source['sentence_num']
            target_sentence_num = target['sentence_num']

            # if source['token_offset'] <= target['token_offset']:
            #     left = source
            #     right = target
            # else:
            #     left = target
            #     right = source
            src_left_indexes, src_right_indexes = self._get_context(source['token_offset'], source['token_offset'],
                                                            len(self.pre_processed_text[src_sentence_num]))
            target_left_indexes, target_right_indexes = self._get_context(target['token_offset'], target['token_offset'],
                                                            len(self.pre_processed_text[target_sentence_num]))
            # source vector, left means "source" here
            left_context = [token['token'] for token in
                            self.pre_processed_text[src_sentence_num][src_left_indexes[0]:src_right_indexes[1]]]
            # target vector, right means "target" here
            right_context = [token['token'] for token in
                            self.pre_processed_text[target_sentence_num][target_left_indexes[0]:target_right_indexes[1]]]

            id_pair_to_context[(src_id, target_id)] = (left_context, right_context)
        return id_pair_to_context

    def get_t0_subpaths(self):
        t0_path = {}
        for item in self.dct_pairs:
            # get the first word from the sentence
            entity_id = item[0]
            sent_num = self.id_to_sent[entity_id]
            # first_id = self.pre_processed_text[sent_num][0]['id']
            #
            # entity_wordID = self.id_to_wordIDs[entity_id][0]
            #
            # left_path, right_path = self.dependency_paths.get_left_right_subpaths('t'+first_id[1:], 't'+entity_wordID[1:])
            # t0_path[entity_id] = right_path

            entity_wordID = self.id_to_wordIDs[entity_id][0]
            root_wordID = self.find_root(entity_wordID)

            # get path between the entity and sentence root
            left, right = self.dependency_paths.get_left_right_subpaths(root_wordID, 't'+entity_wordID[1:])
            if len(left) > len(right):
                t0_path[entity_id] = left
            else:
                t0_path[entity_id] = right

        return t0_path

    def get_t0_context(self):
        t0_path = {}
        context = []
        for item in self.dct_pairs:
            # get the first word from the sentence
            entity_id = item[0]

            entity_wordID = self.id_to_wordIDs[entity_id][0]
            entity = self.id_to_tok[entity_wordID]
            sentence_num = entity['sentence_num']

            left_indexes, right_indexes = self._get_context(entity['token_offset'], entity['token_offset'],
                                                                    len(self.pre_processed_text[sentence_num]))
            context = [token['token'] for token in
                            self.pre_processed_text[sentence_num][left_indexes[0]:right_indexes[1]]]
            t0_path[entity_id] = context

        return t0_path

    def get_id_to_labels(self):
        """Get map from id pairs to labels"""
        raw_tlinks = self.get_tlinks()
        self.id_to_labels = {}
        for item in raw_tlinks:
            if 'eventInstanceID' in item.attrib:
                src_id = self.eiid_to_eid[item.attrib['eventInstanceID']]
            elif 'timeID' in item.attrib:
                src_id = item.attrib['timeID']
            if 'relatedToEventInstance' in item.attrib:
                target_id = self.eiid_to_eid[item.attrib['relatedToEventInstance']]
            elif 'relatedToTime' in item.attrib:
                target_id = item.attrib['relatedToTime']
            self.id_to_labels[(src_id, target_id)] = item.attrib.get('relType', 'None')

        # self.labeled_pairs_indexes = []
        #
        # # self.all_pairs = sorted(self.intra_sentence_pairs + self.cross_sentence_pairs + self.dct_pairs)
        #
        # for i, pair in enumerate(self.all_pairs):
        #     if pair in self.id_to_labels:
        #         self.labeled_pairs_indexes.append(i) # indexes of all valid pairs with true labels
        # self.labeled_pairs_indexes = np.array(self.labeled_pairs_indexes)

        return self.id_to_labels

    def get_neg_pairs(self):
        pass

    def get_labels(self):
        """
        @return: a list of dict lists. Every item in the list corresponds to a list of tokens.
                 every token {'entity_id': 'e1', 'entity_label': 'ASPECTUAL', 'entity_type': 'EVENT'}
                 has an entity label, which is '0' if not an labeled entity
        """

        if self.annotated_note_path is not None and self.iob_labels == []:

            # don't want to modify original
            pre_processed_text = copy.deepcopy(self.pre_processed_text)

            # need to create a list of tokens
            iob_labels = []

            tagged_entities = get_tagged_entities(self.annotated_note_path)
            _tagged_entities = copy.deepcopy(tagged_entities)

            raw_text = get_text(self.note_path)
            labeled_text = get_text_with_taggings(self.annotated_note_path)

            # lots of checks!
            for char in ['\n'] + list(whitespace):
                raw_text     = raw_text.strip(char)
                labeled_text = labeled_text.strip(char)

            raw_text     = re.sub(r"``", r"''", raw_text)
            labeled_text = re.sub(r'"', r"'", labeled_text)

            raw_text = re.sub("<TEXT>\n*", "", raw_text)
            raw_text = re.sub("\n*</TEXT>", "", raw_text)

            labeled_text = re.sub("<TEXT>\n*", "", labeled_text)
            labeled_text = re.sub("\n*</TEXT>", "", labeled_text)

            raw_index = 0
            labeled_index = 0

            raw_char_offset = 0
            labeled_char_offset = 0

            # should we count?
            count_raw = True
            count_labeled = True

            text1 = ""
            text2 = ""

            start_count = 0
            end_count = 0

            offsets = {}

            tagged_element = None

            # need to get char based offset for each tagging within annotated timeml doc.
            while raw_index < len(raw_text) or labeled_index < len(labeled_text):

                if raw_index < len(raw_text):
                    if count_raw is True:
                        raw_char_offset += 1
                        text1 += raw_text[raw_index]
                    raw_index += 1

                if labeled_index < len(labeled_text):

                    # TODO: change this to be an re match.
                    if labeled_text[labeled_index:labeled_index+1] == '<' and labeled_text[labeled_index:labeled_index+2] != '</':

                        tagged_element = tagged_entities.pop(0)

                        count_labeled = False
                        start_count += 1

                    elif labeled_text[labeled_index:labeled_index+2] == '</':
                        count_labeled = False
                        start_count += 1

                    if labeled_text[labeled_index:labeled_index+1] == ">":

                        if tagged_element != None:

                            start = labeled_char_offset
                            try:
                                span = len(tagged_element.text)
                            except TypeError: # sometimes tagged_element.text is None
                                span = 0
                            end = labeled_char_offset + span - 1

                            # spans should be unique?
                            offsets[(start, end)] = {"tagged_xml_element":tagged_element, "text":tagged_element.text}

                            # ensure the text at the offset is correct
                            # try:
                            #     assert raw_text[start:end + 1] == tagged_element.text, "\'{}\' != \'{}\'".format( raw_text[start:end + 1], tagged_element.text)
                            # except AssertionError:
                            #     print raw_text[start:end + 1]
                            #     print tagged_element.text
                            #     sys.exit("AssertionError")
                            tagged_element = None

                        end_count += 1
                        count_labeled = True

                        labeled_index += 1
                        continue

                    if count_labeled is True:
                        labeled_char_offset += 1
                        text2 += labeled_text[labeled_index]

                    labeled_index += 1

            try:
                assert text1 == text2, "{} != {}".format(text1, text2)
            except AssertionError:
                print "lengths:", len(text1), len(text2)
                if len(text2) > len(text1):
                    print "last chars:", text1[-5:] + '<END>', text2[-5:] + '<END>'
                for i,char in enumerate(text1):
                    char2 = text2[i]
                    if char != char2:
                        print "found difference at position %s:" % i
                        print "in text1:", char
                        print "in text2:", char2
                        sys.exit()
                sys.exit()

            assert start_count == end_count, "{} != {}".format(start_count, end_count)
            assert raw_index == len(raw_text) and labeled_index == len(labeled_text)
            assert raw_char_offset == labeled_char_offset
            assert len(tagged_entities) == 0
            assert tagged_element is None
            assert len(offsets) == len(_tagged_entities)

            for sentence_num in sorted(pre_processed_text.keys()):

                # list of dicts
                sentence = pre_processed_text[sentence_num]

                # iobs in a sentence
                iobs_sentence = []

                # need to assign the iob labels by token index
                for token in sentence:


                    # set proper iob label to token
                    iob_label, entity_type, entity_id, entity_value = TimeNote.get_label(token, offsets)

                    if iob_label is not 'O':
                        assert entity_id is not None
                        assert entity_type in ['EVENT', 'TIMEX3']
                    else:
                        assert entity_id is None
                        assert entity_type is None



                    #if token["token"] == "expects":
                    #    print "Found expects"
                    #    print "iob_label: ", iob_label
                    #    print "entity_type: ", entity_type
                    #    print "entity_id: ", entity_id
                    #    print
                    #    sys.exit("done")

                    iobs_sentence.append({'entity_label':iob_label,
                                          'entity_type':entity_type,
                                          'entity_id':entity_id,
                                          'entity_value':entity_value})

                iob_labels.append(iobs_sentence)

            self.iob_labels = iob_labels

        return self.iob_labels

    def get_tokens(self):

        tokens = []

        for line in self.pre_processed_text:

            for token in self.pre_processed_text[line]:

                tokens.append(token)

        return tokens

    def set_iob_labels(self, iob_labels):

        # don't over write existing labels.
        assert len(self.iob_labels) == 0

        self.iob_labels = iob_labels

    def get_tlink_ids(self):

        tlink_ids = []

        for tlink in self.tlinks:

            tlink_ids.append(tlink["tlink_id"])

        return tlink_ids

    def get_tlink_labels(self):
        """ return the labels of each tlink from annotated doc """

        tlink_labels = []

        for tlink in self.tlinks:

            tlink_labels.append(tlink["rel_type"])

        return tlink_labels

    def get_tlink_id_pairs(self):

        """ returns the id pairs of two entities joined together """

        tlink_id_pairs = []

        for tlink in self.tlinks:

            tlink_id_pairs.append((tlink["src_id"], tlink["target_id"]))

        return tlink_id_pairs

    def get_token_char_offsets(self):

        """ returns the char based offsets of token.

        for each token within self.pre_processed_text iterate through list of dicts
        and for each value mapped to the key 'start_offset' and 'end_offset' create a
        list of 1-1 mappings

        Returns:
            A flat list of offsets of the token within self.pre_processed_text:

                [(0,43),...]
        """

        offsets = []

        for line_num in self.pre_processed_text:
            for token in self.pre_processed_text[line_num]:
                offsets.append((token["char_start_offset"], token["char_end_offset"]))

        return offsets

    def get_tokens_from_ids(self, ids):
        ''' returns the token associated with a specific id'''
        tokens = []
        for _id in ids:
            # ensuring id prefix value is correct.
            # TODO: adjust TimeNote to consistently use t# or w# format
            tokens.append(self.id_to_tok['w' + _id[1:]]["token"])
        return tokens

    def write(self, timexEventLabels, tlinkLabels, idPairs, offsets, tokens, output_path):
        '''
        Note::write()

        Purpose: add annotations this notes tml file and write new xml tree to a .tml file in the output folder.

        params:
            timexEventLabels: list of dictionaries of labels for timex and events.
            tlinkLabels: list labels for tlink relations
            idPairs: list of pairs of eid or tid that have a one to one correspondance with the tlinkLabels
            offsets: list of offsets tuples used to locate events and timexes specified by the label lists. Have one to one correspondance with both lists of labels.
            tokens: tokens in the note (used for tense)
            output_path: directory to write the file to
        '''

        # TODO: create output directory if it does not exist
        root = get_stripped_root(self.note_path)
        length = len(offsets)
        doc_time = get_doctime_timex(self.note_path).attrib["value"]

        # hack so events are detected in next for loop.
        for label in timexEventLabels:
            if label["entity_label"][0:2] not in ["B_","I_","O"] or label["entity_label"] in ["I_STATE", "I_ACTION"]:
                label["entity_label"] = "B_" + label["entity_label"]

        # start at back of document to preserve offsets until they are used
        for i in range(1, length+1):
            index = length - i

            if timexEventLabels[index]["entity_label"][0:2] == "B_":
                start = offsets[index][0]
                end = offsets[index][1]
                entity_tokens = tokens[index]["token"]

                #grab any IN tokens and add them to the tag text
                for j in range (1, i):

                    if(timexEventLabels[index + j]["entity_label"][0:2] == "I_"):
                        end = offsets[index + j][1]
                        entity_tokens += ' ' + tokens[index + j]["token"]
                    else:
                        break

                if timexEventLabels[index]["entity_type"] == "TIMEX3":
                    # get the time norm value of the time expression
                    # timex_value = get_normalized_time_expressions(doc_time, [entity_tokens])
                    timex_value = ''

                    # if no value was returned, set the expression to an empty string
                    # TODO: check if TimeML has a specific default value we should use here
                    if len(timex_value) != 0:
                        timex_value = timex_value[0]
                    else:
                        timex_value = ''

                   # if None in [start, end,  timexEventLabels[index]["entity_id"], timexEventLabels[index]["entity_label"][2:], timex_value]:
                   #     print "FOUND NoNE"
                   #     print [start, end,  timexEventLabels[index]["entity_id"], timexEventLabels[index]["entity_label"][2:], timex_value]
        #          #      exit()
                   # else:
                   #     print "NONE NONE"
                   #     print [start, end,  timexEventLabels[index]["entity_id"], timexEventLabels[index]["entity_label"][2:], timex_value]


                    annotated_text = annotate_text_element(root, "TIMEX3", start, end, {"tid": timexEventLabels[index]["entity_id"], "type":timexEventLabels[index]["entity_label"][2:], "value":timex_value})
                else:
                    annotated_text = annotate_text_element(root, "EVENT", start, end, {"eid": timexEventLabels[index]["entity_id"], "class":timexEventLabels[index]["entity_label"][2:]})
                    #if None in [start, end,  timexEventLabels[index]["entity_id"], timexEventLabels[index]["entity_label"][2:], timex_value]:
                    #    print "FOUND NoNE"
                    #    print [start, end,  timexEventLabels[index]["entity_id"], timexEventLabels[index]["entity_label"][2:], timex_value]
        #                exit()
                    #else:
                    #    print "NONE NONE"
                    #    print [start, end,  timexEventLabels[index]["entity_id"], timexEventLabels[index]["entity_label"][2:], timex_value]

                set_text_element(root, annotated_text)

        # make event instances
        eventDict = {}
        for i, timexEventLabel in enumerate(timexEventLabels):

            token = tokens[i]

            pos = None

            # pos
           # if token["pos_tag"] == "IN":
           #     pos = "PREPOSITION"
           # elif token["pos_tag"] in ["VB", "VBD","VBG", "VBN", "VBP", "VBZ", "RB", "RBR", "RBS"]:
           #     pos = "VERB"
           # elif token["pos_tag"] in ["NN", "NNS", "NNP", "NNPS", "PRP", "PRP$"]:
           #     pos = "NOUN"
           # elif token["pos_tag"] in ["JJ", "JJR", "JJS"]:
           #     pos = "ADJECTIVE"
           # else:
           #     pos = "OTHER"

            if timexEventLabel["entity_type"] == "EVENT":
                #root = annotate_root(root, "MAKEINSTANCE", {"eventID": timexEventLabel["entity_id"], "eiid": "ei" + str(i), "tense":"NONE", "pos":"NONE"})
                root = annotate_root(root, "MAKEINSTANCE",
                                     {"eventID": timexEventLabel["entity_id"], "eiid": "ei" + timexEventLabel["entity_id"][1:], "tense": "NONE",
                                      "pos": "NONE"})
                #eventDict[timexEventLabel["entity_id"]] = "ei" + str(i)
                eventDict[timexEventLabel["entity_id"]] = "ei" + timexEventLabel["entity_id"][1:]

        # add tlinks
        for i, tlinkLabel in enumerate(tlinkLabels):

            if tlinkLabel == "None":
                continue

            annotations = {"lid": "l" + str(i), "relType": tlinkLabel}

            firstID = idPairs[i][0]
            secondID = idPairs[i][1]

            if firstID[0] == "e":
                annotations["eventInstanceID"] = eventDict[firstID]

            if firstID[0] == "t":
                annotations["timeID"] = firstID

            if secondID[0] == "e":
                annotations["relatedToEventInstance"] = eventDict[secondID]

            if secondID[0] == "t":
                annotations["relatedToTime"] = secondID

            root = annotate_root(root, "TLINK", annotations)

        note_path = os.path.join(output_path, self.note_path.split('/')[-1] + ".tml")

        print "root: ", root
        print "note_path: ", note_path

        write_root_to_file(root, note_path)

    @staticmethod
    def get_label(token, offsets):

        # NOTE: never call this directly. input is tested within _read
        tok_span = (token["char_start_offset"], token["char_end_offset"])

        label = 'O'
        entity_id = None
        entity_type = None
        entity_value = None

        for span in offsets:

            if offsets[span]["tagged_xml_element"].tag not in ["EVENT", "TIMEX3"]:
                continue

            if TimeNote.same_start_offset(span, tok_span):

                labeled_entity = offsets[span]["tagged_xml_element"]

                if 'class' in labeled_entity.attrib:
                    label = 'B_' + labeled_entity.attrib["class"]  # e.g. B_OCCURRENCE
                elif 'type' in labeled_entity.attrib:
                    label = 'B_' + labeled_entity.attrib["type"]   # e.g. B_DATE

                if 'eid' in labeled_entity.attrib:
                    entity_id = labeled_entity.attrib["eid"]
                else:
                    entity_id = labeled_entity.attrib["tid"]

                if 'value' in labeled_entity.attrib:
                    entity_value = labeled_entity.attrib["value"]

                # TODO: There are other attributes, which may be useful in the future

                entity_type = labeled_entity.tag # EVENT or TIMEX3

                break

            elif TimeNote.subsumes(span, tok_span):

                labeled_entity = offsets[span]["tagged_xml_element"]

                if 'class' in labeled_entity.attrib:
                    label = 'I_' + labeled_entity.attrib["class"]
                else:
                    label = 'I_' + labeled_entity.attrib["type"]

                if 'eid' in labeled_entity.attrib:
                    entity_id = labeled_entity.attrib["eid"]
                else:
                    entity_id = labeled_entity.attrib["tid"]
                if 'value' in labeled_entity.attrib:
                    entity_value = labeled_entity.attrib["value"]

                entity_type = labeled_entity.tag

                break

        if entity_type == "EVENT":
            # don't need iob tagging just what the type is.
            # multi token events are very rare.
            label = label[2:]    # remove B_ I_

        return label, entity_type, entity_id, entity_value

    @staticmethod
    def same_start_offset(span1, span2):
        """
        doees span1 share the same start offset?
        """
        return span1[0] == span2[0]

    @staticmethod
    def subsumes(span1, span2):
        """
        does span1 subsume span2?
        """
        return span1[0] < span2[0] and span2[1] <= span1[1]


def __unit_tests():

    """ basic assertions to ensure output correctness """

    t =  TimeNote("APW19980219.0476.tml.TE3input", "APW19980219.0476.tml")

    for label in t.get_timex_iob_labels():
        for token in label:

            if token['entity_type'] == 'TIMEX3':
                assert token['entity_label'] != 'O'
            else:
                assert token['entity_label'] == 'O'

    for label in t.get_event_iob_labels():
        for token in label:

            if token['entity_type'] == 'EVENT':
                assert token['entity_label'] != 'O'
            else:
                assert token['entity_label'] == 'O'

    """
    number_of_tlinks = len(t.get_tlink_features())
    assert number_of_tlinks != 0
    assert len(t.get_tlink_id_pairs()) == number_of_tlinks, "{} != {}".format(len(t.get_tlink_id_pairs()), number_of_tlinks)
    assert len(t.get_tlink_labels()) == number_of_tlinks
    assert len(t.get_tlink_ids()) == number_of_tlinks
    #prin t.get_token_char_offsets()
    """

    t.get_tlink_features()

#    print t.get_iob_features()

#    print t.get_tlinked_entities()

#    print t.get_tlink_labels()


if __name__ == "__main__":

    __unit_tests()

    print "nothing to do"




